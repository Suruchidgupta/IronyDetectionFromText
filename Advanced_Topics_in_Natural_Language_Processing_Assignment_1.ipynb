{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1RtZxYj6gjin"
   },
   "source": [
    "### Detection of Irony and Sarcasm in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjOGcWfwgjir"
   },
   "source": [
    "This notebook implements a system to detect irony and sarcasm in text. The test data used is from SemEval-2018 task on irony detection\n",
    "\n",
    "Submitted By: Suruchi Gupta\n",
    "Student Id: 19233027\n",
    "Course Code: 1MAI1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf86_sJygjis"
   },
   "source": [
    "# Task 1 (5 Marks)\n",
    "\n",
    "Read all the data and find the size of vocabulary of the dataset (ignoring case) and the number of positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1673,
     "status": "ok",
     "timestamp": 1582998911418,
     "user": {
      "displayName": "Suruchi Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgR_TEEed9ER1kFlRSLYGzqNLhDzUvQygcwKu-d=s64",
      "userId": "02336803466427660677"
     },
     "user_tz": 0
    },
    "id": "zGWjoO91gjit",
    "outputId": "82801344-c208-4c0a-c766-33901b4acc1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/suruchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary packages for preprocessing of text\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2628,
     "status": "ok",
     "timestamp": 1582998912387,
     "user": {
      "displayName": "Suruchi Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgR_TEEed9ER1kFlRSLYGzqNLhDzUvQygcwKu-d=s64",
      "userId": "02336803466427660677"
     },
     "user_tz": 0
    },
    "id": "jVED460xhPLD",
    "outputId": "6968e082-c7bc-4cd2-cc49-47154043612f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Class Label</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sweet united nations video. just in time for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 we are rumored to have talked to erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>hey there! nice to see you minnesota/nd winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left i'm dying over here\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>\"i can't breathe!\" was chosen as the most nota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>you're never too old for footie pajamas. http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>nothing makes me happier then getting on the h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4:30 an opening my first beer now gonna be a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>@adam_klug do you think you would support a gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>@samcguigan544 you are not allowed to open tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>oh, thank god - our entire office email system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>but instead, i'm scrolling through facebook, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>@targetzonept :pouting_face: no he bloody isn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>cold or warmth both suffuse one's cheeks with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>just great when you're mobile bill arrives by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>crushes are great until you realize they'll ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>buffalo sports media is smarter than all of us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>i guess my cat also lost 3 pounds when she wen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>@yankeeswfan @ken_rosenthal trading a sp for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>but @darklightdave was trying to find us, and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  Class Label                                              Tweet\n",
       "0      1            1  sweet united nations video. just in time for c...\n",
       "1      2            1  @mrdahl87 we are rumored to have talked to erv...\n",
       "2      3            1  hey there! nice to see you minnesota/nd winter...\n",
       "3      4            0              3 episodes left i'm dying over here\\n\n",
       "4      5            1  \"i can't breathe!\" was chosen as the most nota...\n",
       "5      6            0  you're never too old for footie pajamas. http:...\n",
       "6      7            1  nothing makes me happier then getting on the h...\n",
       "7      8            0  4:30 an opening my first beer now gonna be a l...\n",
       "8      9            0  @adam_klug do you think you would support a gu...\n",
       "9     10            0  @samcguigan544 you are not allowed to open tha...\n",
       "10    11            1  oh, thank god - our entire office email system...\n",
       "11    12            0  but instead, i'm scrolling through facebook, i...\n",
       "12    13            0  @targetzonept :pouting_face: no he bloody isn'...\n",
       "13    14            0  cold or warmth both suffuse one's cheeks with ...\n",
       "14    15            1  just great when you're mobile bill arrives by ...\n",
       "15    16            1  crushes are great until you realize they'll ne...\n",
       "16    17            1  buffalo sports media is smarter than all of us...\n",
       "17    18            0  i guess my cat also lost 3 pounds when she wen...\n",
       "18    19            1  @yankeeswfan @ken_rosenthal trading a sp for a...\n",
       "19    20            1  but @darklightdave was trying to find us, and ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the index, class labels and tweets from the input file and storing in a dataframe\n",
    "vocabulary = []\n",
    "def build_vocab():\n",
    "    vocab_dict = {\"Index\": [], \"Class Label\": [], \"Tweet\": []}\n",
    "\n",
    "    file = open(\"SemEval2018-T3-train-taskA.txt\", encoding=\"UTF-8\")\n",
    "    data =  file.readlines()\n",
    "    del data[0] #deleting header row\n",
    "    \n",
    "    for row in data:\n",
    "      cells = row.split('\\t')\n",
    "      class_label = int(cells[1])\n",
    "      vocab_dict[\"Index\"].append(cells[0])\n",
    "      vocab_dict[\"Class Label\"].append(class_label)\n",
    "      tweet = str.lower(cells[2])\n",
    "      vocab_dict[\"Tweet\"].append(tweet)\n",
    "      cells[0] = int(cells[0])\n",
    "      cells[1] = class_label\n",
    "      cells[2] = word_tokenize(tweet)\n",
    "      vocabulary.append(tuple(cells))\n",
    "    return pd.DataFrame(vocab_dict)\n",
    "\n",
    "data = build_vocab()\n",
    "\n",
    "# Printing sample data\n",
    "data[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2621,
     "status": "ok",
     "timestamp": 1582998912388,
     "user": {
      "displayName": "Suruchi Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgR_TEEed9ER1kFlRSLYGzqNLhDzUvQygcwKu-d=s64",
      "userId": "02336803466427660677"
     },
     "user_tz": 0
    },
    "id": "tV-MkFM1mA8g",
    "outputId": "a57769df-aaf7-4b95-c341-81fb0f01e962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  13442\n"
     ]
    }
   ],
   "source": [
    "# Finding the size of vocabulary of the data\n",
    "raw_data = data\n",
    "tweets = raw_data[\"Tweet\"].str.cat(sep=\" \")\n",
    "unique_words = word_tokenize(tweets)\n",
    "unique_words = set(unique_words)\n",
    "print(\"Length of vocabulary: \",len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2612,
     "status": "ok",
     "timestamp": 1582998912388,
     "user": {
      "displayName": "Suruchi Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgR_TEEed9ER1kFlRSLYGzqNLhDzUvQygcwKu-d=s64",
      "userId": "02336803466427660677"
     },
     "user_tz": 0
    },
    "id": "kS6Eheqogji2",
    "outputId": "dfdc361d-bc41-443c-fec3-eef928e1f7fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive examples:  1911  and negative examples:  1923\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of negative and positive examples\n",
    "positive_exs = raw_data.loc[raw_data['Class Label'] == 1]\n",
    "negative_exs = raw_data.loc[raw_data['Class Label'] == 0]\n",
    "print(\"Number of positive examples: \",len(positive_exs),\" and negative examples: \",len(negative_exs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the number of positive and negative examples we can see that the data is not skewed and has equal number of both positive and negative cases for effective learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_MNE7U1cunFY"
   },
   "source": [
    "# Task 2 (15 Marks)\n",
    "Divide the data into a training and test set and justify your split.\n",
    "\n",
    "Implement a function that calculates the precision, recall and F-Measure for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dividing the data, we will use 70% of total data as training data. This gives training phase ample amount of \n",
    "data to learn and avoids overfitting that can happen if the model learns the data \n",
    "and not the underlying characteristics. Overfitting can also lead to poor performance in the testing phase. \n",
    "The remaining 30% of the data will be used for testing the performance of the model built.\n",
    "\n",
    "For dividing the data into training and test set we will use train_test_split() available in scikitlearn package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-pQaWbmmBhg"
   },
   "outputs": [],
   "source": [
    "# Vectorizing the data and splitting into train and test sets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Using CountVectorizer to retrieve feature vectors from the text\n",
    "vectorizer = CountVectorizer(analyzer = 'word')\n",
    "features = vectorizer.fit_transform(raw_data[\"Tweet\"])\n",
    "X_train, X_test, y_train, y_test  = train_test_split(features, raw_data[\"Class Label\"], train_size=0.70, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5fd2M0e6j5E"
   },
   "outputs": [],
   "source": [
    "# Adding the method to calculate accuracy, precision, recall and F-score for the task\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "def evaluation_metrics(y_test,y_pred):\n",
    "    print(\"Accuracy: \",round(accuracy_score(y_test,y_pred), 3))\n",
    "    print(\"Classification Report: \\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUzBIO6r6bkS"
   },
   "source": [
    "# Task 3 (15 Marks)\n",
    "\n",
    "Suggest some features to extract from each sentence. Implement a simple log-linear model to classify tweets as ironic or not ironic.\n",
    "\n",
    "Train this method and evaluate the results using precision, recall and F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtkXIYywmBkr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.626\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       583\n",
      "           1       0.62      0.63      0.62       568\n",
      "\n",
      "    accuracy                           0.63      1151\n",
      "   macro avg       0.63      0.63      0.63      1151\n",
      "weighted avg       0.63      0.63      0.63      1151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Training a simple Logistic Regression model and evaluating the same using method defined above\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg = log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "evaluation_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N94ivMU61e0"
   },
   "source": [
    "# Task 4 (25 Marks)\n",
    "\n",
    "Develop an acceptor or a transducer recurrent neural network that classifiers the sentence as ironic or not ironic.\n",
    "\n",
    "Evaluate this according to precision, recall or F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6547,
     "status": "error",
     "timestamp": 1582998916344,
     "user": {
      "displayName": "Suruchi Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgR_TEEed9ER1kFlRSLYGzqNLhDzUvQygcwKu-d=s64",
      "userId": "02336803466427660677"
     },
     "user_tz": 0
    },
    "id": "-KROF3Hl63od",
    "outputId": "f9afcbe5-2ec0-4ae0-96c9-c95c08a59163"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 2683 samples, validate on 1151 samples\n",
      "Epoch 1/15\n",
      "2683/2683 [==============================] - 6s 2ms/step - loss: 0.6649 - accuracy: 0.5926 - val_loss: 0.6509 - val_accuracy: 0.6125\n",
      "Epoch 2/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.4193 - accuracy: 0.8196 - val_loss: 0.7203 - val_accuracy: 0.6438\n",
      "Epoch 3/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.1376 - accuracy: 0.9542 - val_loss: 1.0527 - val_accuracy: 0.6273\n",
      "Epoch 4/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.0457 - accuracy: 0.9866 - val_loss: 1.3127 - val_accuracy: 0.6212\n",
      "Epoch 5/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.0252 - accuracy: 0.9948 - val_loss: 1.3371 - val_accuracy: 0.6195\n",
      "Epoch 6/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.0158 - accuracy: 0.9974 - val_loss: 1.4861 - val_accuracy: 0.6212\n",
      "Epoch 7/15\n",
      "2683/2683 [==============================] - 6s 2ms/step - loss: 0.0057 - accuracy: 0.9993 - val_loss: 1.8109 - val_accuracy: 0.6221\n",
      "Epoch 8/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0695 - val_accuracy: 0.6264\n",
      "Epoch 9/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 9.1409e-04 - accuracy: 1.0000 - val_loss: 2.1114 - val_accuracy: 0.6177\n",
      "Epoch 10/15\n",
      "2683/2683 [==============================] - 4s 2ms/step - loss: 6.2047e-04 - accuracy: 1.0000 - val_loss: 2.2951 - val_accuracy: 0.6186\n",
      "Epoch 11/15\n",
      "2683/2683 [==============================] - 4s 2ms/step - loss: 4.5808e-04 - accuracy: 1.0000 - val_loss: 2.4268 - val_accuracy: 0.6108\n",
      "Epoch 12/15\n",
      "2683/2683 [==============================] - 4s 2ms/step - loss: 3.6526e-04 - accuracy: 1.0000 - val_loss: 2.4888 - val_accuracy: 0.6099\n",
      "Epoch 13/15\n",
      "2683/2683 [==============================] - 4s 2ms/step - loss: 2.9028e-04 - accuracy: 1.0000 - val_loss: 2.5833 - val_accuracy: 0.6142\n",
      "Epoch 14/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 2.4321e-04 - accuracy: 1.0000 - val_loss: 2.6631 - val_accuracy: 0.6099\n",
      "Epoch 15/15\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 2.0126e-04 - accuracy: 1.0000 - val_loss: 2.7321 - val_accuracy: 0.6125\n",
      "1151/1151 [==============================] - 0s 398us/step\n",
      "Score on Test Data:  2.732117034786168\n",
      "Accuracy on Test Data:  0.6125108599662781\n"
     ]
    }
   ],
   "source": [
    "# Developing a Recurrent Neural Network to classify texts as ironic or not ironic \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Activation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizing and padding the data to be used by the word embeddings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(raw_data[\"Tweet\"])\n",
    "X = tokenizer.texts_to_sequences(raw_data[\"Tweet\"])\n",
    "X = pad_sequences(X, maxlen=35)\n",
    "\n",
    "# Splitting data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,raw_data[\"Class Label\"], train_size=0.70, random_state=28)\n",
    "\n",
    "# Building the keras sequential model and adding required layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(unique_words), 128))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Initialising the optimizer and building the model\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model \n",
    "model.fit(X_train, y_train,batch_size=28,epochs=15,validation_data=(X_test, y_test))\n",
    "score, accuracy = model.evaluate(X_test, y_test,batch_size=28)\n",
    "print(\"Score on Test Data: \", score)\n",
    "print(\"Accuracy on Test Data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZucGZmL6JM"
   },
   "source": [
    "# Task 5 (40 Marks)\n",
    "\n",
    "Suggest an improvement to either the system developed in Task 3 or 4 and show that it improves according to your evaluation metric.\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10), technical correctness (5) and improvement in evaluation metric (5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For improving the performance of the model, one of the most important step is to improve the quality of the input data. Here, we know that the Tweets given can be preprocessed to improve the model performance. One of the steps for the same can be removing the URLs in the text as it does not contribute to detection of irony in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qP6S0Dj2q7Va"
   },
   "outputs": [],
   "source": [
    "# Removing the URLs from the tweets for preprocessing \n",
    "import re\n",
    "\n",
    "def removing_URL(data):\n",
    "    final_data = []\n",
    "    for text in data:\n",
    "        text = re.sub(r\"https?://.+\",\"\",text)\n",
    "        final_data.append(str.lower(text))\n",
    "    return final_data\n",
    "\n",
    "processed_raw_data = removing_URL(raw_data[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bidirectional LSTMs as an extension to traditional LSTM can improve the classification performance. The bidirectional LSTMs train the data in both left-to-right and right-to-left direction to get a better context and faster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2683 samples, validate on 1151 samples\n",
      "Epoch 1/15\n",
      "2683/2683 [==============================] - 14s 5ms/step - loss: 0.6933 - accuracy: 0.5132 - val_loss: 0.6859 - val_accuracy: 0.5673\n",
      "Epoch 2/15\n",
      "2683/2683 [==============================] - 11s 4ms/step - loss: 0.5543 - accuracy: 0.7279 - val_loss: 0.6893 - val_accuracy: 0.6108\n",
      "Epoch 3/15\n",
      "2683/2683 [==============================] - 10s 4ms/step - loss: 0.2024 - accuracy: 0.9214 - val_loss: 1.0433 - val_accuracy: 0.5769\n",
      "Epoch 4/15\n",
      "2683/2683 [==============================] - 10s 4ms/step - loss: 0.0649 - accuracy: 0.9761 - val_loss: 1.2860 - val_accuracy: 0.5786\n",
      "Epoch 5/15\n",
      "2683/2683 [==============================] - 10s 4ms/step - loss: 0.0404 - accuracy: 0.9862 - val_loss: 1.5940 - val_accuracy: 0.5899\n",
      "Epoch 6/15\n",
      "2683/2683 [==============================] - 11s 4ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 1.7027 - val_accuracy: 0.5847\n",
      "Epoch 7/15\n",
      "2683/2683 [==============================] - 11s 4ms/step - loss: 0.0217 - accuracy: 0.9914 - val_loss: 1.7882 - val_accuracy: 0.5743\n",
      "Epoch 8/15\n",
      "2683/2683 [==============================] - 11s 4ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 2.1129 - val_accuracy: 0.5847\n",
      "Epoch 9/15\n",
      " 400/2683 [===>..........................] - ETA: 8s - loss: 0.0106 - accuracy: 0.9975"
     ]
    }
   ],
   "source": [
    "# Developing a Recurrent Neural Network to classify texts as ironic or not ironic (Using Bidirectional LSTM)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, LSTM, Embedding, Activation, Dropout, Bidirectional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Tokenizing and padding the data to be used by the word embeddings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(processed_raw_data)\n",
    "X = tokenizer.texts_to_sequences(processed_raw_data)\n",
    "X = pad_sequences(X, maxlen=35)\n",
    "\n",
    "# Splitting data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,raw_data[\"Class Label\"], train_size=0.70)\n",
    "\n",
    "# Building the keras sequential model and adding required layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(unique_words), 128))\n",
    "model.add((LSTM(128, return_sequences=True, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(layers.Dense(10, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Initialising the optimizer and building the model\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model \n",
    "model.fit(X_train, y_train,batch_size=25,epochs=15,validation_data=(X_test, y_test))\n",
    "score, accuracy = model.evaluate(X_test, y_test,batch_size=25)\n",
    "print(\"Score on Test Data: \", score)\n",
    "print(\"Accuracy on Test Data: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced_Topics_in_Natural_Language_Processing_Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
